{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca2745236f9b4d34af7ebec38b5ea8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53605602ceea41148c4d405d2f5eff30",
              "IPY_MODEL_15a734757ada48c187e79fa2ac035c04",
              "IPY_MODEL_e7d9b55838754e9297b87b4eeb3ed740"
            ],
            "layout": "IPY_MODEL_915eb57c332e40f8b81c49dc9ad7c796"
          }
        },
        "53605602ceea41148c4d405d2f5eff30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc680f52b684f4781e334cca106cc38",
            "placeholder": "​",
            "style": "IPY_MODEL_745778dd27ff4e57bd9ba8dd367ef533",
            "value": "Batches: 100%"
          }
        },
        "15a734757ada48c187e79fa2ac035c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faad7db49cc3436b85e388cfdf655a7c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c0d592ba75e434b8a4624676c0f9c34",
            "value": 1
          }
        },
        "e7d9b55838754e9297b87b4eeb3ed740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_327ff7c7c1f6488fb33a9330d2299f2e",
            "placeholder": "​",
            "style": "IPY_MODEL_b6b6757356eb4063901a76283cf74365",
            "value": " 1/1 [00:00&lt;00:00,  9.20it/s]"
          }
        },
        "915eb57c332e40f8b81c49dc9ad7c796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc680f52b684f4781e334cca106cc38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "745778dd27ff4e57bd9ba8dd367ef533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faad7db49cc3436b85e388cfdf655a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0d592ba75e434b8a4624676c0f9c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "327ff7c7c1f6488fb33a9330d2299f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b6757356eb4063901a76283cf74365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# URL of the lecture notes\n",
        "url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "# Fetch the content from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "main_content_div = soup.find('div', class_='main-content', id='main-content')\n",
        "\n",
        "html_content = \"\"\n",
        "\n",
        "if main_content_div:\n",
        "    # Step 3: Extract the inner HTML of the\n",
        "    html_content = main_content_div.decode_contents()\n",
        "\n",
        "# Initialize a dictionary to store scraped data\n",
        "scraped_data = {\n",
        "    'h2_sections': [],\n",
        "    'paragraphs': [],\n",
        "    'tables': [],\n",
        "    'links': [],\n",
        "    'equations': []\n",
        "}\n",
        "\n",
        "# Extracting  sections\n",
        "h2_sections = soup.find_all('h2')\n",
        "for h2 in h2_sections:\n",
        "    scraped_data['h2_sections'].append(h2.text.strip())\n",
        "\n",
        "# Extracting  sections\n",
        "paragraphs = soup.find_all('p')\n",
        "for p in paragraphs:\n",
        "    scraped_data['paragraphs'].append(p.text.strip())\n",
        "\n",
        "# Extracting  sections (if any)\n",
        "tables = soup.find_all('table')\n",
        "for table in tables:\n",
        "    # Convert table to a string representation for storage (if needed)\n",
        "    scraped_data['tables'].append(str(table))\n",
        "\n",
        "# Extracting  tags\n",
        "links = soup.find_all('a')\n",
        "for link in links:\n",
        "    scraped_data['links'].append({\n",
        "        'href': link.get('href'),\n",
        "        'text': link.text.strip()\n",
        "    })\n",
        "\n",
        "# Extracting LaTeX equations\n",
        "latex_pattern = r'$(.*?)$|\\\\§'\n",
        "equations = re.findall(latex_pattern, html_content)\n",
        "for equation in equations:\n",
        "    # 'equation' is now a tuple, so we need to select the first non-empty string\n",
        "    equation_text = next(filter(None, equation)) if isinstance(equation, tuple) else equation\n",
        "    scraped_data['equations'].append(equation_text.strip())\n",
        "\n",
        "# Serialize scraped data to JSON\n",
        "json_data = json.dumps(scraped_data, indent=4)\n",
        "\n",
        "# Write JSON data to a file\n",
        "with open('scraped_datav1.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(\"Data has been scraped and stored in scraped_data.json.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UNxlkrIalJkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8405bb5e-abc8-4290-efe9-c9e90264eaac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been scraped and stored in scraped_data.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# URL of the lecture notes\n",
        "url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "# Fetch the content from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "main_content_div = soup.find('div', class_='main-content', id='main-content')\n",
        "\n",
        "# Initialize a dictionary to store scraped data\n",
        "scraped_data = {}\n",
        "\n",
        "if main_content_div:\n",
        "    # Extract all  tags\n",
        "    h2_sections = main_content_div.find_all('h2')\n",
        "\n",
        "    for h2 in h2_sections:\n",
        "        section_title = h2.text.strip()\n",
        "        section_data = {\n",
        "            'paragraphs': [],\n",
        "            'tables': [],\n",
        "            'links': [],\n",
        "            'equations': [],\n",
        "            'ordered_lists': [],\n",
        "            'unordered_lists': []\n",
        "        }\n",
        "\n",
        "        # Extract paragraphs under current\n",
        "        paragraphs = h2.find_next_siblings('p')\n",
        "        for p in paragraphs:\n",
        "            section_data['paragraphs'].append(p.text.strip())\n",
        "\n",
        "        # Extract tables under current\n",
        "        tables = h2.find_next_siblings('table')\n",
        "        for table in tables:\n",
        "            section_data['tables'].append(str(table))\n",
        "\n",
        "        # Extract links under current\n",
        "        links = h2.find_next_siblings('a')\n",
        "        for link in links:\n",
        "            section_data['links'].append({\n",
        "                'href': link.get('href'),\n",
        "                'text': link.text.strip()\n",
        "            })\n",
        "\n",
        "        # Extract LaTeX equations under current section\n",
        "        latex_pattern = r'$(.*?)$|\\\\§'\n",
        "        equations = h2.find_next_siblings(string=re.compile(latex_pattern))\n",
        "        for equation in equations:\n",
        "            if isinstance(equation, str):\n",
        "                equation_texts = re.findall(latex_pattern, equation)\n",
        "                for eq in equation_texts:\n",
        "                    # Ensure that eq contains a non-None value\n",
        "                    equation_text = next((text for text in eq if text), None)\n",
        "                    if equation_text:  # Only append if a valid equation text was found\n",
        "                        section_data['equations'].append(equation_text.strip())\n",
        "\n",
        "        # Extract ordered lists (ol) under current\n",
        "        ordered_lists = h2.find_next_siblings('ol')\n",
        "        for ol in ordered_lists:\n",
        "            if ol.find_previous_sibling('h2') == h2:\n",
        "                list_items = ol.find_all('li')\n",
        "                list_data = [li.text.strip() for li in list_items]\n",
        "                section_data['ordered_lists'].append(list_data)\n",
        "\n",
        "        # Extract unordered lists (ul) under current\n",
        "        unordered_lists = h2.find_next_siblings('ul')\n",
        "        for ul in unordered_lists:\n",
        "            if ul.find_previous_sibling('h2') == h2:\n",
        "                list_items = ul.find_all('li')\n",
        "                list_data = [li.text.strip() for li in list_items]\n",
        "                section_data['unordered_lists'].append(list_data)\n",
        "\n",
        "        # Add current section data to main dictionary under the current  key\n",
        "        scraped_data[section_title] = section_data\n",
        "\n",
        "# Serialize scraped data to JSON\n",
        "json_data = json.dumps(scraped_data, indent=4)\n",
        "\n",
        "# Write JSON data to a file\n",
        "with open('scraped_data.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(\"Data has been scraped and stored in scraped_data.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcuATOB1och5",
        "outputId": "e357819e-0928-4b0b-a504-899280ec0453"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been scraped and stored in scraped_data.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# URL of the lecture notes\n",
        "url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "# Fetch the content from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "main_content_div = soup.find('div', class_='main-content', id='main-content')\n",
        "\n",
        "# Initialize a dictionary to store scraped data\n",
        "scraped_data = {}\n",
        "current_h2_title = \"\"\n",
        "\n",
        "if main_content_div:\n",
        "    # Extract all  and  tags\n",
        "    h2_h3_sections = main_content_div.find_all(['h2', 'h3'])\n",
        "\n",
        "    for index, tag in enumerate(h2_h3_sections):\n",
        "        # Determine the current section title and type (h2 or h3)\n",
        "        # section_title = tag.text.strip()\n",
        "        # section_type = tag.name\n",
        "        if tag.name == 'h2':\n",
        "            current_h2_title = tag.text.strip()\n",
        "            section_title = current_h2_title\n",
        "        elif tag.name == 'h3':\n",
        "            section_title = f\"{tag.text.strip()} {current_h2_title}\"\n",
        "\n",
        "        # Initialize section data for the current  or  section\n",
        "        section_data = {\n",
        "            'paragraphs': [],\n",
        "            'tables': [],\n",
        "            'links': [],\n",
        "            'equations': [],\n",
        "            'ordered_lists': [],\n",
        "            'unordered_lists': []\n",
        "        }\n",
        "\n",
        "        # Find next  or  tag, or end of siblings\n",
        "        next_tag = h2_h3_sections[index + 1] if index + 1 < len(h2_h3_sections) else None\n",
        "\n",
        "        # Extract content between current tag and next  or\n",
        "        current_tag = tag.find_next_sibling()\n",
        "        while current_tag and (current_tag.name != 'h2' and current_tag.name != 'h3' and current_tag != next_tag):\n",
        "            if current_tag.name == 'p':\n",
        "                section_data['paragraphs'].append(current_tag.text.strip())\n",
        "            elif current_tag.name == 'table':\n",
        "                section_data['tables'].append(str(current_tag))\n",
        "            elif current_tag.name == 'a':\n",
        "                section_data['links'].append({\n",
        "                    'href': current_tag.get('href'),\n",
        "                    'text': current_tag.text.strip()\n",
        "                })\n",
        "            elif current_tag.name and re.match(r'(div|ul|ol|h[1-6])', current_tag.name):\n",
        "                # Check if current_tag.name is not None and matches the specified tags\n",
        "                section_data['equations'].append(current_tag.text.strip())\n",
        "\n",
        "            # Move to the next sibling\n",
        "            current_tag = current_tag.find_next_sibling()\n",
        "            scraped_data[section_title] = section_data"
      ],
      "metadata": {
        "id": "0k-y3dvspbz8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# URL of the lecture notes\n",
        "url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "# Fetch the content from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "main_content_div = soup.find('div', class_='main-content', id='main-content')\n",
        "\n",
        "# Initialize a dictionary to store scraped data\n",
        "scraped_data_1 = {}\n",
        "\n",
        "if main_content_div:\n",
        "    # Extract all <strong> tags\n",
        "    strong_tags = main_content_div.find_all('strong')\n",
        "\n",
        "    for strong_tag in strong_tags:\n",
        "        # Initialize the data for this <strong> tag\n",
        "        strong_text = strong_tag.text.strip()\n",
        "        strong_data = {\n",
        "            'paragraphs': [],\n",
        "            'tables': [],\n",
        "            'links': [],\n",
        "            'equations': [],\n",
        "            'ordered_lists': [],\n",
        "            'unordered_lists': []\n",
        "        }\n",
        "\n",
        "        # Find parent <p> tag of the <strong> tag\n",
        "        parent_p_tag = strong_tag.find_parent('p')\n",
        "        if parent_p_tag:\n",
        "            # Include the parent <p> tag's text\n",
        "            strong_data['paragraphs'].append(parent_p_tag.text.strip())\n",
        "\n",
        "            # Include previous and next sibling <p> tags\n",
        "            previous_sibling = parent_p_tag.find_previous_sibling('p')\n",
        "            next_sibling = parent_p_tag.find_next_sibling('p')\n",
        "\n",
        "            if previous_sibling:\n",
        "                strong_data['paragraphs'].append(previous_sibling.text.strip())\n",
        "\n",
        "            if next_sibling:\n",
        "                strong_data['paragraphs'].append(next_sibling.text.strip())\n",
        "\n",
        "        # Add the data to the main dictionary under the <strong> tag text as key\n",
        "        scraped_data_1[strong_text] = strong_data\n",
        "\n",
        "# Initialize the main dictionary for merging (empty or preloaded)\n",
        "scraped_data = {}\n",
        "\n",
        "# Merge data from scraped_data_1 into scraped_data\n",
        "for key in scraped_data_1:\n",
        "    if key in scraped_data:\n",
        "        for sub_key in scraped_data_1[key]:\n",
        "            if sub_key in scraped_data[key]:\n",
        "                # Extend lists for matching keys\n",
        "                if isinstance(scraped_data[key][sub_key], list):\n",
        "                    scraped_data[key][sub_key].extend(scraped_data_1[key][sub_key])\n",
        "            else:\n",
        "                # Add new keys directly\n",
        "                scraped_data[key][sub_key] = scraped_data_1[key][sub_key]\n",
        "    else:\n",
        "        # Add the entire key if it doesn't exist in scraped_data\n",
        "        scraped_data[key] = scraped_data_1[key]\n",
        "\n",
        "# Serialize merged data back into JSON format\n",
        "merged_json = json.dumps(scraped_data, indent=4)\n",
        "\n",
        "# Save merged JSON data to a file\n",
        "with open('scraped_data.json', 'w') as json_file:\n",
        "    json_file.write(merged_json)\n",
        "\n",
        "print(\"Merged data has been saved to scraped_data.json.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65VzY-ypfoh",
        "outputId": "5c32dce7-8203-419a-b14d-97f75a95772d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data has been saved to scraped_data.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu transformers sentence-transformers\n",
        "!pip install markdown2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWTjyQj1xJwX",
        "outputId": "1968af59-f9b0-4365-e191-f76eb81a7195"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: markdown2 in /usr/local/lib/python3.10/dist-packages (2.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from markdown2 import markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Load your JSON data\n",
        "with open('scraped_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# # Prepare data for embedding and retrieval\n",
        "keys = list(data.keys())\n",
        "texts = []\n",
        "\n",
        "for content in data.values():\n",
        "    paragraphs = content.get('paragraphs', [])\n",
        "    ordered_lists = sum(content.get('ordered_lists', []), [])\n",
        "    unordered_lists = sum(content.get('unordered_lists', []), [])\n",
        "    tables = sum(content.get('tables', []), [])\n",
        "    links = sum(content.get('links', []), [])\n",
        "    equations = content.get('equations', [])\n",
        "\n",
        "    # Concatenate all text elements\n",
        "    text_content = \" \".join(paragraphs + ordered_lists + unordered_lists + tables + equations + links)\n",
        "\n",
        "#     keys.append(content['title'])  # Assuming 'title' is a key in your scraped data for section titles\n",
        "    texts.append(text_content)\n",
        "\n",
        "print(f\"Loaded {len(keys)} pieces of text from the JSON data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8HwTR7XxVQR",
        "outputId": "75232e4a-b90a-4217-ea97-28b6eab93794"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 79 pieces of text from the JSON data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to get embeddings\n",
        "def get_embeddings(texts):\n",
        "    return model.encode(texts)\n",
        "\n",
        "# Generate embeddings for texts and keys\n",
        "text_embeddings = get_embeddings(texts)\n",
        "key_embeddings = get_embeddings(keys)\n",
        "print(f\"Generated embeddings for {len(text_embeddings)} pieces of text and {len(key_embeddings)} keys.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaLulE0V2AIq",
        "outputId": "9bc72666-ece4-4e2c-c387-1d8591442c21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings for 79 pieces of text and 79 keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "key_embeddings = np.array(key_embeddings)\n",
        "\n",
        "# Initialize FAISS index for texts and keys\n",
        "text_index = faiss.IndexFlatL2(text_embeddings.shape[1])\n",
        "key_index = faiss.IndexFlatL2(key_embeddings.shape[1])\n",
        "\n",
        "text_index.add(text_embeddings)\n",
        "key_index.add(key_embeddings)\n",
        "\n",
        "# Save indices\n",
        "faiss.write_index(text_index, 'text_vector_db.index')\n",
        "faiss.write_index(key_index, 'key_vector_db.index')"
      ],
      "metadata": {
        "id": "L8aZNh9e26RJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample data: keys and their corresponding texts\n",
        "data = {\n",
        "    \"Section1\": \"This is text for section 1.\",\n",
        "    \"Section2\": \"Details about section 2 are mentioned here.\",\n",
        "    \"Section3\": \"Section 3 discusses advanced topics.\",\n",
        "}\n",
        "keys = list(data.keys())\n",
        "texts = list(data.values())\n",
        "\n",
        "# Initialize the embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other models too\n",
        "\n",
        "# Generate embeddings for the texts\n",
        "print(\"Generating embeddings...\")\n",
        "embeddings = np.array(model.encode(texts, show_progress_bar=True)).astype('float32')\n",
        "\n",
        "# Build a FAISS index\n",
        "print(\"Building the FAISS index...\")\n",
        "dimension = embeddings.shape[1]  # Embedding size\n",
        "key_index = faiss.IndexFlatL2(dimension)  # Use L2 (Euclidean distance)\n",
        "key_index.add(embeddings)  # Add embeddings to the index\n",
        "\n",
        "# Save the index to a file\n",
        "faiss_index_path = '/content/key_vector_db.index'\n",
        "faiss.write_index(key_index, faiss_index_path)\n",
        "print(f\"Index saved to {faiss_index_path}\")\n",
        "\n",
        "# Query processing function\n",
        "def process_query(query, key_index, top_k=2):\n",
        "    # Generate query embedding\n",
        "    query_embedding = np.array(model.encode([query])).astype('float32')\n",
        "\n",
        "    # Search for similar keys\n",
        "    distances, indices = key_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Retrieve the most relevant keys and their content\n",
        "    result_keys = []\n",
        "    combined_result_content = defaultdict(list)\n",
        "\n",
        "    for idx in indices[0]:\n",
        "        if idx == -1:  # Handle cases with no results\n",
        "            continue\n",
        "        result_key = keys[idx]\n",
        "        result_keys.append(result_key)\n",
        "        result_content = data[result_key]\n",
        "\n",
        "        # Combine content\n",
        "        combined_result_content[result_key].append(result_content)\n",
        "\n",
        "    # Convert defaultdict to a regular dictionary\n",
        "    combined_result_content = dict(combined_result_content)\n",
        "    combined_result_keys = ', '.join(result_keys)\n",
        "\n",
        "    return combined_result_keys, combined_result_content\n",
        "\n",
        "# Example usage: Query the index\n",
        "query = \"advanced topics\"\n",
        "print(\"\\nProcessing query...\")\n",
        "result_keys, result_content = process_query(query, key_index)\n",
        "print(f\"Relevant sections: {result_keys}\")\n",
        "print(\"\\nContent:\", result_content)\n",
        "\n",
        "# Reload index from file\n",
        "print(\"\\nTesting index reload...\")\n",
        "reloaded_index = faiss.read_index(faiss_index_path)\n",
        "result_keys, result_content = process_query(query, reloaded_index)\n",
        "print(f\"Relevant sections (after reload): {result_keys}\")\n",
        "print(\"\\nContent:\", result_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847,
          "referenced_widgets": [
            "ca2745236f9b4d34af7ebec38b5ea8e3",
            "53605602ceea41148c4d405d2f5eff30",
            "15a734757ada48c187e79fa2ac035c04",
            "e7d9b55838754e9297b87b4eeb3ed740",
            "915eb57c332e40f8b81c49dc9ad7c796",
            "1cc680f52b684f4781e334cca106cc38",
            "745778dd27ff4e57bd9ba8dd367ef533",
            "faad7db49cc3436b85e388cfdf655a7c",
            "1c0d592ba75e434b8a4624676c0f9c34",
            "327ff7c7c1f6488fb33a9330d2299f2e",
            "b6b6757356eb4063901a76283cf74365"
          ]
        },
        "id": "KjQjNt5V2-Hh",
        "outputId": "65e39932-6683-495d-c311-765711f16e35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca2745236f9b4d34af7ebec38b5ea8e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the FAISS index...\n",
            "Index saved to /content/key_vector_db.index\n",
            "\n",
            "Processing query...\n",
            "Relevant sections: Section3, Section1\n",
            "\n",
            "Content: {'Section3': ['Section 3 discusses advanced topics.'], 'Section1': ['This is text for section 1.']}\n",
            "\n",
            "Testing index reload...\n",
            "Relevant sections (after reload): Section3, Section1\n",
            "\n",
            "Content: {'Section3': ['Section 3 discusses advanced topics.'], 'Section1': ['This is text for section 1.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a text generation pipeline (e.g., GPT-2)\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "def generate_structured_response(query, result_keys, result_content):\n",
        "    \"\"\"\n",
        "    Generate a structured response based on the query, relevant keys, and content.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        result_keys (str): Comma-separated string of the relevant keys.\n",
        "        result_content (dict): Content associated with the relevant keys.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text response.\n",
        "    \"\"\"\n",
        "    # Create a structured prompt\n",
        "    prompt = f\"**Question:** {query}\\n\\n\"\n",
        "    prompt += f\"**Relevant Sections:** {result_keys}\\n\\n\"\n",
        "\n",
        "    # Add content to the prompt\n",
        "    for section, content_list in result_content.items():\n",
        "        prompt += f\"**Section {section}:**\\n\"\n",
        "        for item in content_list:\n",
        "            prompt += f\"- {item}\\n\"\n",
        "        prompt += \"\\n\"\n",
        "\n",
        "    # Add a closing statement\n",
        "    prompt += \"Answer is:\"\n",
        "\n",
        "    # Define max_length to ensure prompt is not excessively long\n",
        "    max_length = min(len(prompt) + 100, 750)\n",
        "\n",
        "    # Generate response using GPT-2 pipeline\n",
        "    response = generator(prompt[:750], max_length=max_length, num_return_sequences=1, truncation=True, pad_token_id=50256)\n",
        "    generated_text = response[0]['generated_text']\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "query = \"advanced topics\"\n",
        "result_keys = \"Section3\"\n",
        "result_content = {\"Section3\": [\"Section 3 discusses advanced topics.\", \"More details on section 3.\"]}\n",
        "\n",
        "response = generate_structured_response(query, result_keys, result_content)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7yQHRSX4jDz",
        "outputId": "49370045-32af-460c-a45e-515ca1b673ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Question:** advanced topics\n",
            "\n",
            "**Relevant Sections:** Section3\n",
            "\n",
            "**Section Section3:**\n",
            "- Section 3 discusses advanced topics.\n",
            "- More details on section 3.\n",
            "\n",
            "Answer is:** You're missing information here.\n",
            "\n",
            "If you had any suggestions, feel free to post them!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_good(generated_text):\n",
        "    # Convert Markdown with LaTeX support to HTML\n",
        "    html_content = markdown(generated_text, extras=[\"fenced-code-blocks\", \"cuddled-lists\", \"footnotes\", \"tables\", \"toc\", \"smarty-pants\", \"mathjax\",\"latex\"])\n",
        "\n",
        "    # Display the rendered HTML as Markdown\n",
        "    display(Markdown(html_content))\n",
        "\n",
        "# Generate structured response\n",
        "query = \"What is Conditional generation in language model?\"\n",
        "result_key, result_content = process_query(query, key_index)\n",
        "\n",
        "# Generate structured response\n",
        "generated_text = generate_structured_response(query, result_key, result_content)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eik1DP7i57A9",
        "outputId": "883d2e3c-ea8b-4416-9833-4b3f288a50b0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Question:** What is Conditional generation in language model?\n",
            "\n",
            "**Relevant Sections:** Section1, Section2\n",
            "\n",
            "**Section Section1:**\n",
            "- This is text for section 1.\n",
            "\n",
            "**Section Section2:**\n",
            "- Details about section 2 are mentioned here.\n",
            "\n",
            "Answer is: **The key concept of Conditional generation is the use of an abstraction that is a collection of elements (as opposed to a list of elements) that contain only the elements that are of the same type as their other elements: - the list and the empty element (for example \"this\").\n",
            "\n",
            "- If you want to call this type of expression, you will have to define a function that calls the method \"callback\". - Conditional Generation is not an abstraction.\n",
            "\n",
            "\n",
            "CODE:\n",
            "\n",
            "- This is the same as in the definition of \"concurrent generation\".\n",
            "\n",
            "- Let us assume as you know, such a new version of Conditional Generation can be obtained. - What does the following type of expression mean? - Let us give the following description of the type: - C is this kind of expression? The variable definition contains\n",
            "\n",
            "c = 'this'\n",
            "\n",
            "- The number of elements (two-sided)\n",
            "\n",
            "- The number of elements (no two-sided)\n",
            "\n",
            "- If you don't know which type of expression is used, we will give this expression in this list. - Type Description: - Type Description: C is this type of expression? C is this type of expression? O returns the list for this collection.\n",
            "\n",
            "- Returns an iterator over these elements.\n",
            "\n",
            "\n",
            "CODE:\n",
            "\n"
          ]
        }
      ]
    }
  ]
}