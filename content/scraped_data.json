{
    "large language models": {
        "paragraphs": [
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "probability distribution over sequences of tokens": {
        "paragraphs": [
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):",
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "vocabulary": {
        "paragraphs": [
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):",
            "Welcome to CS324! This is a new course on understanding and developing large language models.",
            "The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "syntactic knowledge": {
        "paragraphs": [
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "world knowledge": {
        "paragraphs": [
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Generation": {
        "paragraphs": [
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "chain rule of probability": {
        "paragraphs": [
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence.",
            "For example (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "conditional probability distribution": {
        "paragraphs": [
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).",
            "For example (demo):",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "autoregressive language model": {
        "paragraphs": [
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "temperature": {
        "paragraphs": [
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "annealed": {
        "paragraphs": [
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Conditional generation": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "prompt": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "completion": {
        "paragraphs": [
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Information theory": {
        "paragraphs": [
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "entropy": {
        "paragraphs": [
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "any algorithm": {
        "paragraphs": [
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Entropy of English": {
        "paragraphs": [
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.",
            "Shannon also defined cross entropy:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "cross entropy": {
        "paragraphs": [
            "Shannon also defined cross entropy:",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Estimating entropy via language modeling": {
        "paragraphs": [
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Shannon game (human language model)": {
        "paragraphs": [
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).",
            "Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Noisy channel model": {
        "paragraphs": [
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:",
            "Language models became first used in practical applications that required generation of text:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "noisy channel model": {
        "paragraphs": [
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:",
            "Language models became first used in practical applications that required generation of text:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "N-gram models": {
        "paragraphs": [
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).",
            "For example, a trigram (\\(n=3\\)) model would define:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "n-gram model": {
        "paragraphs": [
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).",
            "For example, a trigram (\\(n=3\\)) model would define:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "computationally cheap": {
        "paragraphs": [
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "statistically infeasible": {
        "paragraphs": [
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn\u2019t a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "local dependencies": {
        "paragraphs": [
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn\u2019t a huge problem.",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora):",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "statistically feasible": {
        "paragraphs": [
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "computationally expensive": {
        "paragraphs": [
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).",
            "Since 2003, two other key developments in neural language modeling include:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Recurrent Neural Networks": {
        "paragraphs": [
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "entire context": {
        "paragraphs": [
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Transformers": {
        "paragraphs": [
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made \u201clarge enough\u201d for many applications (GPT-3 used \\(n = 2048\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "easier to train": {
        "paragraphs": [
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made \u201clarge enough\u201d for many applications (GPT-3 used \\(n = 2048\\))."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "large": {
        "paragraphs": [
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Increase in size": {
        "paragraphs": [
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "5000x": {
        "paragraphs": [
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Emergence": {
        "paragraphs": [
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "emergent": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "conditional generation": {
        "paragraphs": [
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Examples of capabilities": {
        "paragraphs": [
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "One can prompt a language model to solve word analogies (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "question answering": {
        "paragraphs": [
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:",
            "One can prompt a language model to solve word analogies (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "word analogies": {
        "paragraphs": [
            "One can prompt a language model to solve word analogies (demo):",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "generate a news article": {
        "paragraphs": [
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "One can prompt a language model to solve word analogies (demo):",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3,": {
        "paragraphs": [
            "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled \u201cStanford Researchers Discover Black Holes in Language Models.\u201d The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition \u2013 and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. \u201cIt\u2019s like if you have a paper cup and you push in the bottom,\u201d said Stanford computer scientist Michael Schmidt. \u201cAt first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.\u201d Schmidt and his colleagues are using computers to identify and avoid semantic black holes."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "In-context learning": {
        "paragraphs": [
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "in-context learning": {
        "paragraphs": [
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let\u2019s start with an example (demo):",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Where is Stanford University? Output:": {
        "paragraphs": [
            "Input: Where is Stanford University? Output: Stanford University is in California."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "examples": {
        "paragraphs": [
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output:": {
        "paragraphs": [
            "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output: Stanford"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Relationship to supervised learning": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "one language model": {
        "paragraphs": [
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Research": {
        "paragraphs": [
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "research": {
        "paragraphs": [
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Industry": {
        "paragraphs": [
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "production": {
        "paragraphs": [
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "affecting billions of people": {
        "paragraphs": [
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "complex": {
        "paragraphs": [
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "substantial risks": {
        "paragraphs": [
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Reliability": {
        "paragraphs": [
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind\u2019s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Input: Who invented the Internet? Output:": {
        "paragraphs": [
            "Input: Who invented the Internet? Output: Al Gore"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Social bias": {
        "paragraphs": [
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "He": {
        "paragraphs": [
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "She": {
        "paragraphs": [
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data": {
        "paragraphs": [
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?",
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):",
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Toxicity": {
        "paragraphs": [
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example:",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?",
            "As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Disinformation": {
        "paragraphs": [
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "Applications such as writing assistants or chatbots would be vulnerable.",
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Security": {
        "paragraphs": [
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "data poisoning": {
        "paragraphs": [
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Legal considerations": {
        "paragraphs": [
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Cost and environmental impact": {
        "paragraphs": [
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "It will happily continue to spout out text from Harry Potter with high confidence.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "expensive": {
        "paragraphs": [
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "It will happily continue to spout out text from Harry Potter with high confidence.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "environmental impact": {
        "paragraphs": [
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Access": {
        "paragraphs": [
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "This course will be structured like an onion:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "closed": {
        "paragraphs": [
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face\u2019s Big Science project, EleutherAI, and Stanford\u2019s CRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.",
            "This course will be structured like an onion:"
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Behavior": {
        "paragraphs": [
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we\u2019ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Data": {
        "paragraphs": [
            "Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don\u2019t have full access to the model."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Building": {
        "paragraphs": [
            "Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.)."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    },
    "Beyond": {
        "paragraphs": [
            "Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models."
        ],
        "tables": [],
        "links": [],
        "equations": [],
        "ordered_lists": [],
        "unordered_lists": []
    }
}